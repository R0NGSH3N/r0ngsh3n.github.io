---
layout: post
title: Use Spark Streaming as ETL tools
tags: Spark, Java, Big Data, ETL, database
categories: common
---

  Spark could be used as ETL tools, today we are going
to walk you throught how to and explain the required Spark knowledge.

## Install Spark

## Spark Overview

`Spark Core`: have to have it

- Fundamental Component
- Task Distribution
- Scheduling
- input/output

`Spark SQL` : we don't need it.

`Spark Streaming`: ***This is our star***

- Streaming analytics
- Micro Batches
- Lambda Architecture

`MLlib` : we dont' need it

## Spark Data:

`RDD` - Resilient Distributed Dataset, `container` that all you to work with data object.

`DataFrame` - data table

`Dataset` - combination of `RDD` and `DataFrame`, ***this is the objects we are going to work with***

## Spark Actions:

`Action` and `Transformation`

`Action` call `Transformation` to do something.

![Picture 1](https://r0ngsh3n.github.io/static/img/1212/Screenshot_spark-etl-1.png)

## Streaming How-To:

Spark Streaming process:

![Picture 2](https://r0ngsh3n.github.io/static/img/1212/Screenshot_spark-etl-2.png)

